{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2025) - Assignment 1\n",
    "\n",
    "**Due: Tuesday, January 21 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/EugeneFrancisco/RL-book-Eugene/tree/master/HW1\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Eugene Francisco\n",
    "- Nanxi Jiang\n",
    "- Hakeem Shindy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Snakes and Ladders (Led by Nanxi Jiang)\n",
    "\n",
    "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we model this problem with a Markov Process?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Transition Probabilities\n",
    "\n",
    "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Modeling the Game\n",
    "\n",
    "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces, and plot the graph of the distribution of time steps to finish the game. Use the image below for the locations of the snakes and ladders.\n",
    "\n",
    "![Snakes and Laddders](./Figures/snakesAndLadders.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "State Space: $\\mathcal{S} = \\{x: x \\in \\mathbb{W}, x \\leq 100\\}$, Terminal Space: $\\mathcal{T} = \\{100\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "For $s = \\{1, 2, ..., 93\\}$, where the state does not contain a the head of a snake or bottom of a ladder, the transition probabilities are: \n",
    "$$\\mathcal{F}(x) = \n",
    "\\begin{cases}\n",
    "x+1 & \\text{with probability } \\frac{1}{6}\\\\\n",
    "x+2 & \\text{with probability } \\frac{1}{6}\\\\\n",
    "x+3 & \\text{with probability } \\frac{1}{6}\\\\\n",
    "x+4 & \\text{with probability } \\frac{1}{6}\\\\\n",
    "x+5 & \\text{with probability } \\frac{1}{6}\\\\\n",
    "x+6 & \\text{with probability } \\frac{1}{6}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "For $s \\in \\{94, 95, 96, 96, 98, 99\\}$,\n",
    "$$\\mathcal{F}(x) = \n",
    "\\begin{cases}\n",
    "94 & \\text{terminates with probability } \\frac{1}{6}\\\\\n",
    "95 & \\text{terminates with probability } \\frac{2}{6}\\\\\n",
    "96 & \\text{terminates with probability } \\frac{3}{6}\\\\\n",
    "97 & \\text{terminates with probability } \\frac{4}{6}\\\\\n",
    "98 & \\text{terminates with probability } \\frac{5}{6}\\\\\n",
    "99 & \\text{terminates with probability } 1\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "For $s \\in \\{\\text{snakes}\\}$,\n",
    "$$\\mathcal{F}(x) = \n",
    "\\begin{cases}\n",
    "snakes[s] & \\text{with probability } 1\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "For $s \\in \\{\\text{ladders}\\}$,\n",
    "$$\\mathcal{F}(x) = \n",
    "\\begin{cases}\n",
    "ladders[s] & \\text{with probability } 1\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code\n",
    "import numpy as np\n",
    "\n",
    "snakes = {32:10, 36:6, 48:26, 63:18, 88:24, 95:56, 97:78}\n",
    "ladders = {1:38, 4:14, 8:30, 21:42, 28:76, 50:67, 71: 92, 88:99}\n",
    "\n",
    "#dictionary of dictionaries\n",
    "transitionMap = {}\n",
    "for i in range(0, 101):\n",
    "    if i in snakes:\n",
    "        transitionMap[i] = {snakes[i]: 1}\n",
    "    elif i in ladders:\n",
    "        transitionMap[i] = {ladders[i]: 1}\n",
    "    elif i > 94:\n",
    "        transitionMap[i] = {}\n",
    "        for j in range (1, 7):\n",
    "            if (i+j < 100):\n",
    "                transitionMap[i].update({i+j:1/6})\n",
    "            else:\n",
    "                transitionMap[i].update({100:(i-93)/6})\n",
    "                break\n",
    "    else:\n",
    "        transitionMap[i] = {}\n",
    "        for j in range (1, 7):\n",
    "            transitionMap[i].update({i+j:1/6})\n",
    "display(transitionMap)\n",
    "\n",
    "#snakesAndLadders = simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Markov Decision Processes (Led by Nanxi Jiang)\n",
    "\n",
    "Consider an MDP with an infinite set of states $\\mathcal{S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by: \n",
    "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in \\mathcal{S} \\text{ for all } a \\in [0,1]$$\n",
    "For all states $s \\in \\mathcal{S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): Optimal Value Function  \n",
    "\n",
    "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in \\mathcal{S}$. Given $V^*(s)$, what is the optimal action, $a^*$, that maximizes the optimal value function?\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Optimal Policy  \n",
    "\n",
    "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in \\mathcal{S}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Changing the Action Space  \n",
    "\n",
    "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
    "\n",
    "- The form of the Bellman optimality equation?\n",
    "- The optimal value function, $V^*(s)$?\n",
    "- The structure of the optimal policy, $\\pi^*(s)$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "Confused about the chronology of this question; where do the rewards go if they are dependent on s'?\n",
    "$$V^*(s) = max(\\mathcal{R}(s,a)+\\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}(s,a,s') \\cdot V^*(s))$$\n",
    "$$V^*(s) = max(\\sum_{k=0}^\\infty (0.5)^k (-2a^2+a+1))$$\n",
    "To optimize, we take the derivative of the quadratic and set it to zero:\n",
    "$$-4a+1 = 0$$\n",
    "$$a = \\frac{1}{4}$$\n",
    "Thus $V^*(s) = max(\\sum_{k=0}^\\infty (0.5)^k \\frac{9}{8} = \\frac{9}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "$\\pi^*(s) = \\frac{1}{4} \\; \\forall s \\in \\mathcal{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "#### Bellman Optimality Equation Change:\n",
    "\n",
    "The action space is now constrained by $\\frac{1}{s}$.\n",
    "\n",
    "#### Optimal Value Function Change:\n",
    "\n",
    "The optimal value function output will decrease since for any $s > 5$, the action is restricted from optimality.\n",
    "\n",
    "#### Optimal Policy Change:\n",
    "\n",
    "Since we want to get as close as possible to $\\frac{1}{4}$ without violating the new state constraint, the new $\\pi^*(s)$ is:\n",
    "$$\\pi^* (s) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{4} & s \\leq 4\\\\\n",
    "\\frac{1}{s} & s > 4\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Frog in a Pond (Led by Eugene Francisco)\n",
    "\n",
    "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**. \n",
    "\n",
    "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
    "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
    "  \n",
    "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
    "\n",
    "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)? \n",
    "\n",
    "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Express the frog-escape problem as an MDP using clear mathematical notation by defining the following components: \n",
    "\n",
    "- **State Space**: Define the possible states of the MDP.\\\\\n",
    "The state space $\\mathcal{S}$ is just the numbers $$\n",
    "- **Action Space**: Specify the actions available to the frog at each state. \n",
    "- **Transition Function**: Describe the probabilities of transitioning between states for each action. \n",
    "- **Reward Function**: Specify the reward associated with the states and transitions. \n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Python Implementation\n",
    "\n",
    "There is starter code below to solve this problem programatically. Fill in each of the $6$ `TODO` areas in the code. As a reference for the transition probabilities and rewards, you can make use of the example in slide 16/31 from the following slide deck: https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-MP.pdf.\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "- Models this MDP.\n",
    "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
    "\n",
    "Feel free to use/adapt code from the textbook. Note, there are other libraries that are needed to actually run this code, so running it will not do anything. Just fill in the code so that it could run assuming that the other libraries are present.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Visualization and Analysis\n",
    "\n",
    "After running the code, we observe the following graphs for $n=3$, $n=10$, and $n=25$:\n",
    "\n",
    "![FrogGraphs](./Figures/frogGraphs.png)\n",
    "\n",
    "What patterns do you observe for the **Optimal Policy** as you vary $n$ from $3$ to $25$? When the frog is on lilypad $13$ (with $25$ total), what action should the frog take? Is this action different than the action the frog should take if it is on lilypad $1$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### State Space:  \n",
    "\n",
    "The space $\\mathcal{S}$ are just the numbers $\\{0,\\ldots, n\\}$ for the different lilypads the frog could be on. Of these, the non-terminating sets $\\mathcal{N}$ are the numbers $\\{1,\\ldots, n - 1\\}\\subset S$, since these are the lilypads for which we keep playing the game.\n",
    "\n",
    "#### Action Space:  \n",
    "\n",
    "There are two possible actions at any lilypad, so $\\mathcal{A} = \\{A, B\\}$, where each action represents the corresponding croak.\n",
    "\n",
    "#### Transition Function:  \n",
    "\n",
    "We are interested in $\\mathcal{P}: \\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{S}\\to [0, 1]$, where $\\mathcal{P}(s, a, s') := \\sum_{r\\in \\mathcal{D}}\\mathcal{P}_\\mathcal{R}(s, a, r, s') = \\mathbb{P}(S_{t+1} = s'| A_t = a, S_t = s)$.\n",
    "\\begin{align*}\n",
    "\t\\mathcal{P}(s, a, s') &= \\mathbb{P}(S_{t + 1} = s' | S_t = s, A_t = a)\\\\\n",
    "\t&=\\begin{cases}\n",
    "\t\ts/n & : A_t = A, s' = s - 1\\\\\n",
    "\t\t(n - s)/n &: A_t = A, s' = s + 1\\\\\n",
    "\t\t0 &: A_t = A, s' \\neq s\\pm 1\\\\\n",
    "\t\t1/n &: A_t = B, s' \\neq s\\\\\n",
    "\t\t0 &: A_t = B, s' = s\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Reward Function:  \n",
    "\n",
    "Since there isn't a given reward structure, we will set up rewards as follows: Whenever an agent transitions to state $s$, they receive reward $s$, the motivation being to incentivize getting to the $n$th lilypad. First, the reward transition function:\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\mathcal{R}_T(s, a, s') &:= \\mathbb{E}(R_{t+1}| (S_{t + 1} = s', S_t = s, A_t = a))\\\\\n",
    "\t&= s'\n",
    "\\end{align*}\n",
    "\n",
    "We note that because of this reward structure, $\\mathcal{P_{\\mathcal{R}}}(s, a, r, s') = \\mathbb{P}((T_{t + 1} = r, S_{t + 1} = s')| S_t = s, A_t = a) = \\mathcal{P}(s, a, s')$, since going to state $s'$ guarantees the reward. \n",
    "\n",
    "Also the reward function:\n",
    "\\begin{align*}\n",
    "\t\\mathcal{R}(s, a) & := \\mathbb{E}(R_{t + 1}| S_t = a, A_t = a)\\\\\n",
    "\t&= \\begin{cases}\n",
    "\t\t\\sum_{s'\\in \\mathcal{S}}r\\mathcal{P}_{\\mathcal{R}}(s, A, r, s')&: a = A\\\\\n",
    "\t\t\\sum_{s'\\in \\mathcal{S}} r\\mathcal{P}_{\\mathcal{R}}(s, B, r, s')&: a = B.\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "By the remark above and our reward setup, this simplifies to\n",
    "\\begin{align*}\n",
    "\t\\mathcal{R}(s, a) &= \\begin{cases}\n",
    "\t\t(s - 1)\\frac{s}{n} + (s + 1)\\frac{n - s}{n} &: a = A\\\\\n",
    "\t\t\\sum_{s'\\in \\mathcal{S}, s'\\neq s} s'\\frac{1}{n} &: a = B\n",
    "\t\\end{cases}\\\\\n",
    "\t&= \\begin{cases}\n",
    "\t\t\\frac{n(s + 1) - 2s}{n} &: a = A\\\\\n",
    "\t\t\\frac{1}{n}\\sum_{s' \\in \\mathcal{S}, s'\\neq s}s' &: a = B\n",
    "\t\\end{cases}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDPRefined = dict\n",
    "def get_lily_pads_mdp(n: int) -> MDPRefined:\n",
    "    data = {\n",
    "        i: {\n",
    "            'A': {\n",
    "                i - 1: i/n, # DONE: fill in with the correct transition probabilities\n",
    "                i + 1: (n - i)/n, # DONE: fill in with the correct transition probabilities\n",
    "            },\n",
    "            'B': {\n",
    "                j: 1/n if j != i else 0 for j in range(0, n + 1) # DONE: fill in with the correct transition probabilities\n",
    "            }\n",
    "        } for i in range(1, n)\n",
    "    }\n",
    "    data[0] = 0 # TODO: this is the initial state, so what would be the correct transition probabilities?\n",
    "    data[n] = 0 # TODO: similarly, this is the terminal state, so what would be the correct transition probabilities?\n",
    "\n",
    "    gamma = 1.0\n",
    "    return MDPRefined(data, gamma)\n",
    "\n",
    "Mapping = dict\n",
    "def direct_bellman(n: int) -> Mapping[int, float]:\n",
    "    vf = [0.5] * (n + 1)\n",
    "    vf[0] = 0.\n",
    "    vf[n] = 0.\n",
    "    tol = 1e-8\n",
    "    epsilon = tol * 1e4\n",
    "    while epsilon >= tol:\n",
    "        old_vf = [v for v in vf]\n",
    "        \n",
    "        for i in range(1, n):\n",
    "            # Sorry, this is a pretty dense piece of code.\n",
    "            # \n",
    "            # The first line (beginning with sum( j * blah)) is the formula for R(s, a)\n",
    "            #\n",
    "            # The second line is the formula for \\sum_{s' \\in N}\n",
    "            vf[i] = max([\n",
    "                sum( j * get_lily_pads_mdp[0][i][a][j] for j in range(n+1) if j in get_lily_pads_mdp[0][i][a])\n",
    "                 + get_lily_pads_mdp[1] * sum( [get_lily_pads_mdp[0][i][a][j]*vf[j] \n",
    "            for j in range(1, n) if j in get_lily_pads_mdp[0][i][a]] )\n",
    "            for a in ['A', 'B'] \n",
    "            ]) # TODO: fill in with the Bellman update\n",
    "        \n",
    "        epsilon = max(abs(old_vf[i] - v) for i, v in enumerate(vf))\n",
    "    return {v: f for v, f in enumerate(vf)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "What we observe is that, regardless of the number of lilypads, the quality of each state (except on the first lilypad) is always higher under action $A$ than under action $B$. \n",
    "\n",
    "This suggests that given a state, we should always pursue action $A$ unless we are on the first lilipad. So if we are on lilypad 13, we should croak $A$, but if we were on lilypad 13, then we should croak $B$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Manual Value Iteration (Led by Eugene Francisco)\n",
    "\n",
    "Consider a simple MDP with $\\mathcal{S} = \\{s_1, s_2, s_3\\}, \\mathcal{T} = \\{s_3\\}, \\mathcal{A} = \\{a_1, a_2\\}$. The State Transition Probability function  \n",
    "$$\\mathcal{P}: \\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{P}(s_1, a_1, s_1) = 0.25, \\mathcal{P}(s_1, a_1, s_2) = 0.65, \\mathcal{P}(s_1, a_1, s_3) = 0.1$$  \n",
    "$$\\mathcal{P}(s_1, a_2, s_1) = 0.1, \\mathcal{P}(s_1, a_2, s_2) = 0.4, \\mathcal{P}(s_1, a_2, s_3) = 0.5$$  \n",
    "$$\\mathcal{P}(s_2, a_1, s_1) = 0.3, \\mathcal{P}(s_2, a_1, s_2) = 0.15, \\mathcal{P}(s_2, a_1, s_3) = 0.55$$  \n",
    "$$\\mathcal{P}(s_2, a_2, s_1) = 0.25, \\mathcal{P}(s_2, a_2, s_2) = 0.55, \\mathcal{P}(s_2, a_2, s_3) = 0.2$$  \n",
    "\n",
    "The Reward Function  \n",
    "$$\\mathcal{R}: \\mathcal{N} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{R}(s_1, a_1) = 8.0, \\mathcal{R}(s_1, a_2) = 10.0$$  \n",
    "$$\\mathcal{R}(s_2, a_1) = 1.0, \\mathcal{R}(s_2, a_2) = -1.0$$  \n",
    "\n",
    "Assume a discount factor of $\\gamma = 1$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) the first two iterations of the Value Iteration algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): 2 Iterations\n",
    "\n",
    "1. Initialize the Value Function for each state to be its $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}(\\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Argument\n",
    "\n",
    "1. Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. *Hint*: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Policy Evaluation\n",
    "\n",
    "1. Using the policy $\\pi_2(\\cdot)$, compute the exact value function $V^{\\pi_2}(s)$ for all $s\\in S$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (D): Sensitivity Analysis\n",
    "\n",
    "Assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$.\n",
    "\n",
    "1. Perform one iteration of Value Iteration starting from the initialized value function $v_0(s)$, where $v_0(s)$ remains the same as in the original problem.\n",
    "2. Determine whether this change impacts the Optimal Deterministic Policy $\\pi(\\cdot)$. If it does, explain why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "**NOTE, markdown doesn't render this as nice as I'd like, so you can also find the .tex file and pdf with these answers in this repo, under HW1 Math Notes.pdf**\n",
    "\n",
    "As the question suggests, we begin with $v_0(s_1) = 10$, $v_0(s_2) = 2$, $v_0(s_3) = 0$. Let's calculate what our greedy policy $\\pi_D^0$ is for this initialized valuation. First, \n",
    "\\begin{align*}\n",
    "\t\\pi_D^0(s_1) = \\text{argmax}_{a \\in A} \\{q_0(s_1, a_1), q_0(s_1, a_2)\\}\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\tq_0(s_1, a_1) &= R(s_1, a_1) + p(s_1, a_1, s_1)v_0(s_1) + p(s_1, a_1, s_2)v_0(s_2)\\\\\n",
    "\t&= 8 + 0.25\\cdot 10 + 0.65 \\cdot 1\\\\\n",
    "\t&= 11.15\n",
    "\\end{align*}\n",
    "while (with a similar calculation)\n",
    "\\begin{align*}\n",
    "\tq_0(s_1, a_2) &= 10 + 0.1\\cdot 10 + 0.4\\cdot 1\\\\\n",
    "\t&= 11.4.\n",
    "\\end{align*}\n",
    "Since $q_0(s_1, a_2)$ is higher, we pick $a_2$ for the strategy when we are on state $s_1$. Similarly\n",
    "\\begin{align*}\n",
    "\tq_0(s_2, a_1) &= 1 + 0.3\\cdot 10 + 0.15 \\cdot 1\\\\\n",
    "\t&= 4.15\\\\\n",
    "\tq_0(s_2, a_2) &= -1 + 0.25\\cdot 10 + 0.55\\cdot 1\\\\\n",
    "\t&= 2.05\n",
    "\\end{align*}\n",
    "so we choose $a_1$ as the policy action for state $s_2$. To recap, \\fbox{$\\pi_D^0(s_1) = a_2, \\pi_D^0(s_2) = a_1$}.\\\\\n",
    "\n",
    "\\textbf{First value iteration:} Note that since the policies are deterministic, $R^{\\pi^0}(s_1) = R(s_1, a_1) = 8$ and $R^{\\pi^0}(s_2) = R(s_2, a_2) = -1$. But this means that the Bellman update equation $v_1(s) = R^{\\pi^0}(s) + \\sum_{s \\in N}p^\\pi(s, s')V_i(s')$ simplifies (because we always take policy $\\pi_D^0(s)$) to\n",
    "\\begin{align*}\n",
    "\tv_1(s) = R(s, \\pi_D^0(s)) + \\sum_{s'\\in N}p(s, \\pi_D^0(s), s')v_0(s')\n",
    "\\end{align*}\n",
    "which is just the quality of taking the greedy action at a state $s$. That means that, as long as our policy is greedy,\n",
    "\\begin{align*}\n",
    "\tv_{t + 1}(s) = q_t(s, \\pi_D^t(s))\n",
    "\\end{align*}\n",
    "which will heavily simplify later iterations. So \\fbox{$v_1(s_1) = 11.4$ and $v_1(s_2) = 4.15$} and $v_1(s_3) = 0$. (Note that $v_k(s_3) = 0$ for all $k$ because it is a terminating state whose value was initialized at 0).\\\\\n",
    "\n",
    "\\textbf{First greedy policy:} As before, we calculate the relevant $q_k(\\cdot, \\cdot)$.\n",
    "\\begin{align*}\n",
    "\t\\pi_D^1(s_1) = \\text{argmax}_{a\\in A}\\{q_1(s_1, a_1), q_1(s_1, a_2)\\}\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\tq_1(s_1, a_1) &= 8 + 0.25\\cdot 11.4 + 0.65 \\cdot 4.15\\\\\n",
    "\t&= 13.55\\\\\n",
    "\tq_1(s_1, a_2) &= 10 + 0.1\\cdot 11.4 + 0.4\\cdot 4.15\\\\\n",
    "\t&= 12.8\n",
    "\\end{align*}\n",
    "so we pick $a_1$ as the policy choice when on state $s_1$. Similarly,\n",
    "\\begin{align*}\n",
    "\t\\pi_D^1(s_2) = \\text{argmax}_{a\\in A}\\{q_1(s_2, a_1), q_1(s_2, a_2)\\}\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\tq_1(s_2, a_1) &= 1 + 0.3\\cdot 11.4 + 0.15 \\cdot 4.15\\\\\n",
    "\t&= 5.04\\\\\n",
    "\tq_1(s_2, a_2) &= -1 + 0.25\\cdot 11.4 + 0.55 \\cdot 4.15\\\\\n",
    "\t&= 4.13\n",
    "\\end{align*}\n",
    "so we pick $a_1$ as the policy pick for state $s_2$. To recap then, \\fbox{$\\pi_D^1(s_1) = \\pi_D^1(s_2) = a_1$}.\\\\\n",
    "\n",
    "\\textbf{Second Value Iteration:} Using the same trick as above, \\fbox{$v_2(s_1) = 12.8$, $v_2(s_2) = 5.04$, and $v_2(s_3) = 0$.}\\\\\n",
    "\n",
    "\\textbf{Second policy iteration:}\n",
    "\\begin{align*}\n",
    "\t\\pi_D^2(s_1) = \\text{argmax}_{a\\in A}\\{q_2(s_1, a_1), q_2(s_1, a_2)\\}\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\tq_2(s_1, a_1) &= 8 + 0.25 \\cdot 12.8 + 0.65 \\cdot 5.04\\\\\n",
    "\t&= 14.48\\\\\n",
    "\tq_2(s_1, a_2) &= 10 + 0.1 \\cdot 12.8 + 0.4 \\cdot 5.04\\\\\n",
    "\t&= 13.30\n",
    "\\end{align*}\n",
    "so we pick $\\pi_D^2(s_1) = a_1$. Similarly,\n",
    "\\begin{align*}\n",
    "\t\\pi_D^2(s_2) = \\text{argmax}_{a\\in A}\\{q_2(s_2, a_1), q_2(s_2, a_2)\\}\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\tq_2(s_2, a_1) &= 1 + 0.3 \\cdot 12.8 + 0.15 \\cdot 5.04\\\\\n",
    "\t&= 5.60\\\\\n",
    "\tq_2(s_2, a_2) &= -1 + 0.25 \\cdot 12.8 + 0.55\\cdot 5.04\\\\\n",
    "\t&= 4.97\n",
    "\\end{align*}\n",
    "so we pick $\\pi_D^2(s_2) = a_1$, ending with \\fbox{$\\pi_D^2(s_1) = \\pi_D^2(s_2) = a_1$.}\n",
    "(For use in the next section, note the final valuation values of $v_3(s_1) = 14.48$, $v_3(s_2) = 5.60$ and $v_3(s_3) = 0$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer:  \n",
    "\n",
    "To show stability, we wish to show that $\\pi_D^k(s) = a_1$ for all $k \\geq 3$. In other words,\n",
    "\\begin{align*}\n",
    "\tq_k(s, a_1) - q_k(s, a_2) > 0.\n",
    "\\end{align*}\n",
    "First, $s_1$:\n",
    "\\begin{align*}\n",
    "\tq_k(s_1, a_1) - q_k(s_1, a_2) &= R(s_1, a_1) - R(s_1, a_2) + \\sum_{s'\\in N}v_k(s')(p(s_1, a_1, s') - p(s_1, a_2, s'))\\\\\n",
    "\t&= -2 + v_k(s_1)(0.25 - 0.1) + v_k(s_2)(0.65 - 0.4)\\\\\n",
    "\t&= -2 + v_k(s_1)(0.15) + v_k(s_2)(0.25)\n",
    "\\end{align*}\n",
    "For $k\\geq 3$, due to the monotonicity of each entry of $v_k$, we have that $v_k(s_1) \\geq v_3(s_1) = 13.88$ and $v_k(s_2) \\geq v_3(s_2) = 5.46$. Substituting these, we get\n",
    "\\begin{align*}\n",
    "\tq_k(s_1, a_1) - q_k(s_1, a_2) &\\geq -2 + 14.48\\cdot(0.15) + 5.60\\cdot(0.25)\\\\\n",
    "\t&= 1.572\\\\\n",
    "\t&>0.\n",
    "\\end{align*}\n",
    "A similar argument works for $s_2$, where\n",
    "\\begin{align*}\n",
    "\tq_k(s_2, a_1) - q_k(s_2, a_2) &= 2 + v_k(s_1)(0.3 - 0.25) + v_k(s_2)(0.15 - 0.55)\\\\\n",
    "\t&= 2 + v_k(s_1)(0.05) + v_k(s_2)(-0.4)\\\\\n",
    "\t&\\geq 2 + 14.48\\cdot(0.05) + 5.60\\cdot(-0.4)\\\\\n",
    "\t&= 0.484\\\\\n",
    "\t&>0.\n",
    "\\end{align*}\n",
    "Which is exactly what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer:  \n",
    "\n",
    "As noted when calculating the valuation functions in part A, because we take action $a_1$ no matter which state we are in, we have that $R^\\pi(s) = R(s, a_1)$, for each $s\\in N$. Also, $p^\\pi(s, s') = P(s, a_1, s')$. Let $v_i = v^{\\pi^2}(s_i)$. Then\n",
    "\\begin{align*}\n",
    "\tv_1 &= R^{\\pi^2}(s) + \\sum_{s\\in N}p^\\pi(s, s')v_s\\\\\n",
    "\t&= R(s_1, a_1) + p(s_1, a_1, a_1)v_1 + p(s_1, a_1, s_2)v_2\n",
    "\\end{align*}\n",
    "and similarly\n",
    "\\begin{align*}\n",
    "\tv_2 = R(s_2, a_1) + p(s_2, a_1, s_1)v_1 + p(s_2, a_1, s_2)v_2.\n",
    "\\end{align*}\n",
    "Substituting in for the values, we get a system of two equations:\n",
    "\\begin{align*}\n",
    "\t-8 &= v_1(-0.75) + v_2(0.65)\\tag{1}\\\\\n",
    "\t-1 &= v_1(0.3) + v_2(-0.85).\\tag{2}\n",
    "\\end{align*}\n",
    "Solved, this yields \\fbox{$v^{\\pi^{2}}(s_1) = 16.84$ and $v^{\\pi^{2}}(s_2) = 7.12$}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "#### Value Iteration:  \n",
    "\n",
    "We begin by calculating what the initial greedy policy is:\n",
    "\\begin{align*}\n",
    "\tq_0(s_1, a_1) &= 8 + 0.25\\cdot 10 + 0.65 \\cdot 1\\\\\n",
    "\t&= 11.15\\\\\n",
    "\tq_0(s_1, a_2) &= 11 + 0.1\\cdot 10 + 0.4 \\cdot 1\\\\\n",
    "\t&= 12.4.\n",
    "\\end{align*}\n",
    "We pick then \\fbox{$\\pi_D^0(s_1) = a_2$ and $v_1(s_1) = 12.4$.} Similarly,\n",
    "\\begin{align*}\n",
    "\tq_0(s_2, a_1) &= 1 + 0.3 \\cdot 10 + 0.15 \\cdot 1\\\\\n",
    "\t&= 4.15\\\\\n",
    "\tq_0(s_2, a_2) &= -1 + 0.25 \\cdot 10 + 0.55 \\cdot 1\\\\\n",
    "\t&= 2.05\n",
    "\\end{align*}\n",
    "giving us \\fbox{$\\pi_D(s_2) = a_1$ and $v_1(s_2) = 4.15$}. Let's calculate the next greedy policy $\\pi_D^1$. First,\n",
    "\\begin{align*}\n",
    "\tq_1(s_1, a_1) = 8 + 12.4\\cdot(0.25) + 4.15(0.65)\\\\\n",
    "\t&= 13.8\\\\\n",
    "\tq_1(s_1, a_2) &= 11 + 12.4\\cdot (0.1) + 4.15\\cdot (0.4)\\\\\n",
    "\t&= 13.9\n",
    "\\end{align*}\n",
    "So we pick \\fbox{$\\pi_D^1(s_1) = a_2$ and $v_2(s_1) = 13.9$.} Similarly\n",
    "\\begin{align*}\n",
    "\tq_1(s_2, a_1) &= 1 + 12.4\\cdot(0.3) + 4.15\\cdot (0.15)\\\\\n",
    "\t&= 5.34\\\\\n",
    "\tq_1(s_2, a_2) &= -1 + 12.4\\cdot 0.25 + 4.15\\cdot 0.55\\\\\n",
    "\t&= 4.38\n",
    "\\end{align*}\n",
    "giving us \\fbox{$\\pi_D^1(s_2) = a_1$ and $v_2(s_2) = 5.34$.}\n",
    "\n",
    "#### Optimal Deterministic Policy:  \n",
    "\n",
    "We claim that this policy is stable and thus optimal. We'll show this in the same way as in part B. Namely, for $k\\geq 2$\n",
    "\\begin{align*}\n",
    "\tq_k(s_1, a_1) - q_k(s_1, a_2) &= R(s_1, a_1) - R(s_1, a_2) + v_k(s_1)(p(s_1, a_1, a_1) - p(s_1, a_2, s_1))\\\\\n",
    "\t&+ v_k(s_2)(p(s_1, a_1, s_2) - p(s_1, a_2, s_2))\\\\\n",
    "\t&= -3 + v_k(s_1)\\cdot(0.15) + v_k(s_2)\\cdot(0.25)\\\\\n",
    "\t&\\geq -3 + 13.9\\cdot (0.15) + 5.34 \\cdot (0.25)\\\\\n",
    "\t&= 0.42\\\\\n",
    "\t&>0\n",
    "\\end{align*}\n",
    "which shows that $\\pi_D^k(s_1) = a_1$ for all $k\\geq 2$. Similarly,\n",
    "\\begin{align*}\n",
    "\tq_k(s_2, a_1) - q_k(s_2, a_2) &= 2 + v_k(s_1)\\cdot(0.05) + v_k(s_2)\\cdot(-0.4)\\\\\n",
    "\t&\\geq 2 + 13.9\\cdot(0.05) + 5.34\\cdot(-0.4)\\\\\n",
    "\t&= 0.559\\\\\n",
    "\t&>0\n",
    "\\end{align*}\n",
    "meaning that $\\pi_D^k(s_2) = a_1$ for all $k\\geq 2$. In other words, the policy stabilizes to the same as it would have been in if we hadn't changed the reward. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Fixed-Point and Policy Evaluation True/False Questions (Led by Hakeem Shindy)\n",
    "\n",
    "### Recall Section: Key Formulas and Definitions\n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "The Bellman Optimality Equation for state-value functions is:\n",
    "$$\n",
    "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^*(s') \\right].\n",
    "$$\n",
    "For action-value functions:\n",
    "$$\n",
    "Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a').\n",
    "$$\n",
    "\n",
    "#### Contraction Property\n",
    "The Bellman Policy Operator $B^\\pi$ is a contraction under the $L^\\infty$-norm:\n",
    "$$\n",
    "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty.\n",
    "$$\n",
    "This guarantees convergence to a unique fixed point.\n",
    "\n",
    "#### Policy Iteration\n",
    "Policy Iteration alternates between:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$.\n",
    "2. **Policy Improvement**: Generate a new policy $\\pi'$ by setting:\n",
    "   $$\n",
    "   \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right].\n",
    "   $$\n",
    "\n",
    "#### Discounted Return\n",
    "The discounted return from time step $t$ is:\n",
    "$$\n",
    "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i,\n",
    "$$\n",
    "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
    "\n",
    "### True/False Questions (Provide Justification)\n",
    "\n",
    "1. **True/False**: If $Q^\\pi(s, a) = 5$, $P(s, a, s') = 0.5$ for $s' \\in \\{s_1, s_2\\}$, and the immediate reward $R(s, a)$ increases by $2$, the updated action-value function $Q^\\pi(s, a)$ also increases by $2$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2. **True/False**: For a discount factor $\\gamma = 0.9$, the discounted return for rewards $R_1 = 5, R_2 = 3, R_3 = 1$ is greater than $6$.\n",
    "\n",
    "---\n",
    "\n",
    "3. **True/False**: The Bellman Policy Operator $B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V$ satisfies the contraction property for all $\\gamma \\in [0, 1)$, ensuring a unique fixed point.\n",
    "\n",
    "---\n",
    "\n",
    "4. **True/False**: In Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ will always perform strictly better than the previous policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "5. **True/False**: If $Q^\\pi(s, a) = 10$ for all actions $a$ in a state $s$, then the corresponding state-value function $V^\\pi(s) = 10$, regardless of the policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "6. **True/False**: The discounted return $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$ converges to a finite value for any sequence of bounded rewards if $\\gamma < 1$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers (Provide justification, brief explanations are fine)\n",
    "\n",
    "#### Question 1:  \n",
    "\n",
    "True. For action value functions we have this equation: $Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a')$. If the immediate reward increases by 2, that changes nothing to the delayed reward half of the equation. Therefore to hold equality the action-value function would also increase by 2.\n",
    "\n",
    "#### Question 2:  \n",
    "\n",
    "True. Looking at this formula for discounted returns: $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$, if we discount accordingly youget a total reward of 8.51 which is . 6. You don't discount the first reward, you discount the second reward once by multiplying with gamma. Then you discount your third reward twice multiplying by gamma squared.\n",
    "\n",
    "#### Question 3:  \n",
    "\n",
    "True. This is a property we talked about in class. In this case gamma acts as the contraction coefficient ensuring that the two V functions V2 and V1 will be closer together than the original functions. Because we know that this function satisfies the contraction property, due to the banache fixed point theorem it also will have a unique fixed-point value function.\n",
    "\n",
    "#### Question 4:  \n",
    "\n",
    "False. The Policy improvement step doesn't always gurantee better performance. Once you reach the optimal policy for example, the polivy improvement step will return the same policy leading to performance being the same and not better.\n",
    "\n",
    "#### Question 5:  \n",
    "\n",
    "True. The state-value function under any policy can also be represented as the expected value of the action-value function. In this example given the probability of getting a 10 is 100% the state-value function will indeed also be 10.\n",
    "\n",
    "#### Question 6:  \n",
    "\n",
    "True. Looking at $G(t)$ we see that we increasingly discount as we approach the future, since $\\gamma^{i - t - 1}\\to 0$ as $i \\to \\infty$, and this is just a geometric series. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
